---
title: "ANÁLISE ESTATÍSTICA MULTIVARIADA"
author: "João Ricardo F. de Lima"
date: "today"
editor: source
lang: pt
language: 
  toc-title-document: '<a href="https://www.facape.br/" target="_blank"><img src="https://github.com/econfacape/macroeconometria/blob/main/logofacape.jpg?raw=true" alt="Logotipo Facape" width="150"></a>'
format: 
  html:
    toc: true
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    toc-location: left
    code-fold: false
    embed-resources: true
    page-layout: full
    fig-asp: 0.618
    fig-width: 8
    fig-height: 5
    fig-dpi: 300
    fig-align: center
    df-print: paged
    fontsize: 13pt
theme:
  light: flatly
execute:
  echo: TRUE
  message: false
  warning: false
---

<hr>

# Análise de Agrupamentos (Cluster)

Segundo Mingoti (2005, p.155), "a Análise de Agrupamentos, também conhecida como Análise de Conglomerados, Classificação ou Cluster, tem como objetivo dividir os elementos da amostra, ou população, em grupos de forma que os elementos pertencentes a um mesmo grupo sejam similares entre si com respeito às variáveis (características) que neles foram medidas, e os elementos em grupos diferentes sejam heterogêneos em relação a estas mesmas características". 

Assim, a idéia básica é maximizar a homogeneidade dentro dos grupos ao mesmo tempo em que se maximiza a heterogeneidade entre os grupos. Para formar grupos de objetos tem-se que medir e comparar a semelhança entre eles. A medida de semelhança fica dificultada quando se considera várias variáveis (características).

No processo de agrupamento, primeiro o conjunto de dados será dividido em grupos com elementos parecidos entre si e diferentes dos outros grupos. Depois os grupos encontrados serão analisados até que efetivamente se encontre a existência de padrões entre eles.

![Etapas da Análise de Agrupamento - Clustering](imgs/cluster1.png){width=100%}


No caso univariado  é possível que apenas uma análise visual ou gráfica dos dados seja suficiente para decidir sobre os agrupamentos de observaçoes. Contudo, isto não é muito mais difícil quando se tem um conjunto maior de variáveis. Para realizar as técnicas de cluster é necessário definir o conceito de distância.

## Etapas da Análise de Cluster

I) Escolher um critério de parecença (similaridade ou dissimilaridade);

II) Formação dos grupos (escolher o algoritmo de agrupamento);

III) Definição de número de grupos: pode ser a posteriori, como resultado da Análise ou a priori, dado o conhecimento ou conveniência da Análise;

IV) Validação do agrupamento: critério do pesquisador;

V) Interpretação e Análise: pode-se fazer estatística descritiva, diferença de médias, etc.

## Medidas de Parecença

Valor numérico que quantifica o grau de semelhança entre um par de objetos. Pode ser de dois tipos: similaridade ou dissimilaridade. Para variáveis quantitativas, quando se buscas agrupar observações as distâncias são as mais usadas. São medidas de **dissimilaridade** entre objetos. O coeficiente de distância assume valor máximo para objetos totalmente diferentes e valor zero para dois objetos idênticos considerando todas as variáveis. 

Em algumas aplicações deseja-se agrupar variáveis ao invés de observações. Neste caso, o coeficiente de correlação de Pearson é mais comumente utilizado. No caso de variáveis binárias, os dados são novamente rearranjados na forma de uma
tabela de contigência, com as variáveis formando as categorias.

Os coeficientes de distância mais comuns têm as seguintes propriedades:

I) *Mínimo Zero*: Se $A=B$, então $D(A,B)=0$;

II) *Positividade*: Se $A \neq B$, então $D(A,B)>0$;

III) *Simetria*: $D(A,B) = D(B,A)$

IV) *Desigualdade triangular*: $D(A,B)+D(B,C) \geq D(A,C)$

Em relação às distâncias sabe-se pelo Teorema de Pitágoras que $c^2=a^2+b^2$, assim, $c=\sqrt{a^2+b^2}$, que é a  **Distância Euclidiana**  entre dois pontos (A e B). Em termos de variáveis, tem-se:

$$
D_{AB}=\sqrt{(X_{2A}-X_{2B})^2+(X_{1B}-X_{1A})^2}
$$
matricialmente, a Distância Euclidiana é definida por $D_{AB}=\sqrt{(X_a-X_b)'(X_a-X_b)}$ e possui as seguintes propriedades:

a) Base geométrica bem definida;

b) Invariante com relação à transformação de origem;

c) Invariante com relação à transformação ortogonal; 

d) Não invariante com relação à transformação de escala;

e) Não invariante com relação à transformação não ortogonal.


A **Distância Euclidiana Quadrática** é dada por

$$
D_{AB}^2=\sum_{j=1}^p(x_{ja}-x_{jb})^2
$$

A **Distância Euclidiana Ponderada** é definida por

$$
D_{AB}=\sqrt{(X_a-X_b)'A(X_a-X_b)}
$$

com A sendo uma matriz de ponderação (pesos diferentes de acordo com a variância das variáveis). Se $A=I$, $D_{AB}$ é a **Distância Euclidiana**. Se $A=S^{-1}$, ou seja, inverso da matriz var-cov, tem-se a **Distância de Mahalanobis**.

Se $A=diag(S_i^2)^{-1}$, em que $S_i^2$ é a variância amostral da i-ésima variável aleatória, leva-se em consideração na ponderação apenas as diferenças de variâncias das variáveis.

Se $A=diag(1/p)$ tem-se a Distância Euclidiana Média;

A **Distância de Minkowsky** é dada por 

$$
D_{AB}=\Big [\sum_i^p w_i | X_{ia} - X_{ib}| ^\lambda  \Big]^{\frac{1}{\lambda}}
$$

onde $w_i$ são pesos de ponderação para as variáveis. Se $\lambda=2$, tem-se a **Distância Euclidiana**. Esta distância é menos afetada pela presença de valores discrepantes numa amostra do que a distância Euclidiana. Se $\lambda=1$, tem-se a **Distância city-block** também conhecida como **Manhattan**.

Para observações representadas através de variáveis qualitativas, é necessária a criação de variáveis binárias, as quais assumem o valor 1 se uma característica de interesse está presente, e 0, caso contrário. Dessa forma, para um par de observações (i; k) medidos através de p variáveis binárias, considere a seguinte Tabela de Contigência:

|              |       | Observação k |     |           |
|:------------:|:-----:|:------------:|:---:|:---------:|
|              |       | 1            | 0   | Total     |
| Observação i | 1     | a            | b   | a+b       |
|              | 0     | c            | d   | c+d       |
|              | Total | a+c          | b+d | p=a+b+c+d |


com base nesta tabela é possível listar algumas medidas de similaridade e a idéia básica de cada uma delas. 

|              |       | 
|:------------:|:-----|
| $\frac{a+d}{p}$  |Pesos iguais para pares 1-1 e 0-0   |
| $\frac{2(a+d)}{2(a+d)+b+c}$ |  Peso em dobro para pares 1-1 e 0-0 |
| $\frac{a+d}{a+d+2(b+c)}$  |  Peso em dobro para pares descasados |
| $\frac{a}{p}$ | Sem pares 0-0 no numerador |
| $\frac{a}{a+b+c}$  | Sem pares 0-0 no numerador e no denominador  |
| $\frac{2a}{2a+b+c}$  | Sem pares 0 - 0 no numerador e no denominador.|
|   | Peso em dobro para pares 1 - 1.    |
| $\frac{a}{a+2(b+c)}$  |  Sem pares 0 - 0 no numerador e no denominador. |
|   | Peso em dobro para pares descasados.  |
| $\frac{a}{b+c}$  | Razão entre pares 1 - 1 e pares descasados. |

Organiza-se a tabela de contingência e calcula-se o coeficiente de similaridade para cada par de observações. Os coeficientes são dispostos em uma matriz simétrica n x n denominada **matriz de similaridade**.

## Técnicas para construção de Conglomerados (Clusters)

Para Mingoti (2005, p.164), "as técnicas de conglomerados ou clusters são frequentemente classificadas em dois tipos: técnicas hierárquicas e não hierárquicas, sendo que as hierárquicas são classificadas em aglomerativas e divisivas".

Os **Métodos hierárquicos aglomerativos** partem do princípio de que no início do processo de agrupamento têm-se n conglomerados (cada elemento é um conglomerado). Em cada passo do algoritmo, os elementos vão sendo agrupados, formando novos conglomerados até o ponto em que todos os elementos estão num único grupo.

Em termos de variabilidade, no estágio inicial é a menor possível e no final, a máxima possível, pois todos os elementos estão agrupados em apenas 1 cluster. Se dois elementos aparecem juntos num mesmo grupo em algum estçgio do processo de agrupamento, permanecerão juntos até o final, ou seja, não podem mais ser separados.

Devido a esta propriedade, denominada hierarquia, se pode construiu o **Dendograma**. Dendograma é um gráfico em forma de árvore no qual a escala vertical indica o nível de similaridade (ou dissimilaridade). No eixo horizontal tem-se os elementos e no vertical a altura correspondente ao nível em que os elementos foram considerados semelhantes. Quanto maior a altura maior a variabilidade, ou seja, maior a heterogeneidade dos grupos a serem unidos.

A outra técnica de construção dos clusters é chamada de **Método hierárquivo Divisivo**,  que se inicia com todos os objetos em um grupo; formam-se subgrupos desagregando os grupos formados até terminar com cada objeto formando um grupo.

Existem ainda os **Métodos não hierárquicos, também denominados de métodos de partição**. Inicia-se com um número pré definido de grupos e a cada passo procura-se realocar os objetos de maneira a encontrar a melhor partição, isto é, a que minimiza a variância dentro do grupo e maximiza a variância entre grupos. Os métodos não hierárquicos não geram dendrograma, são aplicados a casos (observações) e não a variáveis e são mais indicados para amostras grandes.

## Métodos de agrupamento

I) **Método de ligação simples**: também denominado de vizinho mais próximo ou *single linkage*. A similaridade entre dois grupos é definida pelos elementos mais parecidos. É a distância entre os vizinhos mais próximos ou entre os elementos mais parecidos de cada conglomerado.

$$
D_{I,II} = min \quad ({d_{ij}, i \in I, j \in II})
$$

II) **Método de ligação completa ou do vizinho mais distante**: A similaridade entre dois grupos é definida pelos elementos que são "menos semelhantes" entre si. Em cada passo do algoritmo são combinados os elementos que apresentarem o menor valor máximo de distância.


$$
D_{I,II} = max \quad ({d_{ij}, i \in I, j \in II})
$$

III) **Método da média das distâncias** (*average linkage*): Considera a distância entre dois grupos como a média das distâncias entre todos os pares de elementos dos dois grupos que estão sendo comparados.

$$
D_{i,j}=\frac{\sum_{i=1}^{n_I} \sum_{j=1}^{n_{II}}d_{ij}}{n_I n_{II}}
$$

IV) **Método dos centróides**: A distância entre dois grupos é definida como sendo a distância entre os vetores de médias (centróides), dos grupos comparados. É a distância Euclidiana quadrática entre os vetores de médias amostrais.

$$
D_{AB}=(\bar X_a - \bar X_b)'(\bar X_a - \bar X_b)
$$

V) **Método de Ward**: No processo aglomerativo a medida que se agrupa o nível de similaridade diminui. Em cada passo do agrupamento ocorre diminuição de variabilidade entre grupos e aumento dentro dos grupos. O método de Ward se baseia na mudança de variação que ocorre de um estágio para outro. Este método tende a formar grupos com maior homogeneidade interna. O método considera o aumento na soma de quadrados dos erros como critério para juntar dois grupos. Para um dado grupo g, a soma de quadrados dos erros, $SQE_g$, é a soma dos desvios ao quadrado de cada item para a média do grupo (centróide). 

Em um estágio com G grupos, define-se $SQE$ como a soma $SQE=SQE_1+SQE_2+SQE_3+ \dots SQE_G$. Quando se junta dois grupos, SQE aumenta. Em cada etapa, a união de todos os pares de grupos é analisada. Junta-se os dois grupos que resultam no menor aumento de $SQE$

Segundo Mingoti (2005), "os métodos de ligação simples, completa e da média podem ser utilizados tanto para variáveis quantitativas, quanto qualitativas, ao contrário dos métodos do centróide e de Ward que são apropriados apenas para variáveis quantitativas, já que têm como base a comparação de vetores de médias".

## Análise de Conglomerados (Clusters) no R - Método Hieráquico

```  {r estat8, warning=FALSE, message=FALSE}
#Direcionado o R para o Diretorio a ser trabalhado
setwd('/Users/jricardofl/Dropbox/tempecon/dados_gescilene')

#Lendo os dados no R
library(foreign)
library(car)
library(tidyverse)
#library(Rcmdr)
library(corrplot)
library(MVar.pt)
library(cluster)
library(clValid) # Avaliação dos grupos
library(e1071) # Fuzzy K-médias
library(factoextra) # Vizualização de grupos
library(gridExtra) # Ferramentas gráficas
library(ggforce) # Ferramentas gráficas

#Entrada de Dados
dados <- read.dta("dados_gesc.dta")
str(dados)

# Nomes das Variáveis utilizadas 
#x1 Idade
#x2	Escolaridade
#x3	Renda Bruta Total
#x4 Produtividade
#x5 Valor total anual  de mão de obra permanente
#x6	Valor Total com custo anual com insumos agrícolas nas ativ. Irrig. em 2014
#x7	Custo com energia eletrica e agua
#x8 Capital total empregado na ativ irrig em 2014
#x9	N total de empregados
#x10 Indice de introducao de inovacao (III)
#x11 Indice de Inovacoes realizadas (II)
#x12 Gastos para desenvolver as atividade de inovacao sobre faturamento de 2014
#x13 Quantidade de tecnologia agricola utilizada na atividade irrigada	
#x14 Dummy se a empresa realizou atividades de treinamento e capacitacao de recursos humanos entre 2010 a 2014
#x15 Indice de Fonte de Informacao
#x16 Dummy se empresa esteve envolvida em atividades cooperativas entre os anos de 2010 e 2014
#####################################################

#dados <- dados1 %>% select(-x14, -x16)
dados <- dados[,-c(14,16)]

#Se quiser padronizar os dados
dados.pad <-as.data.frame(scale(dados))

#Analise Fatorial
fatorial1 <- factanal(dados.pad, factors=4, rotation="none", na.action=na.omit)
fatorial2 <- factanal(dados.pad, factors=4, rotation="varimax", na.action=na.omit, scores = c("regression"))

#Analise de Cluster
# Função que obtém o WSS para o método hierárquico
# NÃO MUDAR!
get_wss <- function(d, cluster){
  d <- stats::as.dist(d)
  cn <- max(cluster)
  clusterf <- as.factor(cluster)
  clusterl <- levels(clusterf)
  cnn <- length(clusterl)
  
  if (cn != cnn) {
    warning("cluster renumbered because maximum != number of clusters")
    for (i in 1:cnn) cluster[clusterf == clusterl[i]] <- i
    cn <- cnn
  }
  cwn <- cn
  # Compute total within sum of square
  dmat <- as.matrix(d)
  within.cluster.ss <- 0
  for (i in 1:cn) {
    cluster.size <- sum(cluster == i)
    di <- as.dist(dmat[cluster == i, cluster == i])
    within.cluster.ss <- within.cluster.ss + sum(di^2)/cluster.size
  }
  within.cluster.ss
}

# Função que constrói o "gráfico do cotovelo" e aponta o K ótimo
# NÃO MUDAR!
elbow.plot <- function(x, kmax = 15, alg = "kmeans") {
  # alg = c("kmeans", "cmeans", "hclust")
  wss <- c()
  if (alg == "kmeans") {
    for (i in 1:kmax) {
      set.seed(13)
      tmp <- kmeans(x, i)
      # wss[i] <- get_wss(dist(x), tmp$cluster)
      wss[i] <- tmp$tot.withinss
    }
    tmp <- data.frame(k = 1:kmax, wss)
    max_k <- max(tmp$k)
    max_k_wss <- tmp$wss[which.max(tmp$k)]
    max_wss <- max(tmp$wss)
    max_wss_k <- tmp$k[which.max(tmp$wss)]
    max_df <- data.frame(x = c(max_wss_k, max_k), y = c(max_wss, max_k_wss))
    tmp_lm <- lm(max_df$y ~ max_df$x)
    d <- c()
    for(i in 1:kmax) {
      d <- c(d, abs(coef(tmp_lm)[2]*i - tmp$wss[i] + coef(tmp_lm)[1]) /
               sqrt(coef(tmp_lm)[2]^2 + 1^2))
    }
    tmp$d <- d
    ggplot(data = tmp, aes(k, wss)) +
      geom_line() +
      geom_segment(aes(x = k[1], y = wss[1],
                       xend = max(k), yend = wss[which.max(k)]),
                   linetype = "dashed") +
      geom_point(aes(size = (d == max(d)), color = (d == max(d))),
                 show.legend = FALSE) +
      scale_size_manual(values = c(2,5)) +
      scale_color_manual(values = c("black", "red")) +
      labs(x = "Number of clusters",
           y = "Total within-cluster sum of squares",
           title = "Elbow plot for the K-means method") +
      theme_bw()
  }
  else if (alg == "cmeans") {
    for (i in 1:kmax) {
      if (i == 1) {
        wss[i] <- get_wss(dist(x), rep(1, nrow(x)))
      }
      else {
        set.seed(13)
        tmp <- cmeans(x, i)
        wss[i] <- get_wss(dist(x), tmp$cluster)
        # wss[i] <- tmp$sumsqrs$tot.within.ss
      }
    }
    tmp <- data.frame(k = 1:kmax, wss)
    max_k <- max(tmp$k)
    max_k_wss <- tmp$wss[which.max(tmp$k)]
    max_wss <- max(tmp$wss)
    max_wss_k <- tmp$k[which.max(tmp$wss)]
    max_df <- data.frame(x = c(max_wss_k, max_k), y = c(max_wss, max_k_wss))
    tmp_lm <- lm(max_df$y ~ max_df$x)
    d <- c()
    for(i in 1:kmax) {
      d <- c(d, abs(coef(tmp_lm)[2]*i - tmp$wss[i] + coef(tmp_lm)[1]) /
               sqrt(coef(tmp_lm)[2]^2 + 1^2))
    }
    tmp$d <- d
    ggplot(data = tmp, aes(k, wss)) +
      geom_line() +
      geom_segment(aes(x = k[1], y = wss[1],
                       xend = max(k), yend = wss[which.max(k)]),
                   linetype = "dashed") +
      geom_point(aes(size = (d == max(d)), color = (d == max(d))),
                 show.legend = FALSE) +
      scale_size_manual(values = c(2,5)) +
      scale_color_manual(values = c("black", "red")) +
      labs(x = "Number of clusters",
           y = "Total within-cluster sum of squares",
           title = "Elbow plot for the fuzzy K-means method") +
      theme_bw()
  }
  else if (alg == "hclust") {
    for (i in 1:kmax) {
      set.seed(13)
      tmp <- hcut(x, i)
      wss[i] <- get_wss(dist(x), tmp$cluster)
    }
    tmp <- data.frame(k = 1:kmax, wss)
    max_k <- max(tmp$k)
    max_k_wss <- tmp$wss[which.max(tmp$k)]
    max_wss <- max(tmp$wss)
    max_wss_k <- tmp$k[which.max(tmp$wss)]
    max_df <- data.frame(x = c(max_wss_k, max_k), y = c(max_wss, max_k_wss))
    tmp_lm <- lm(max_df$y ~ max_df$x)
    d <- c()
    for(i in 1:kmax) {
      d <- c(d, abs(coef(tmp_lm)[2]*i - tmp$wss[i] + coef(tmp_lm)[1]) /
               sqrt(coef(tmp_lm)[2]^2 + 1^2))
    }
    tmp$d <- d
    ggplot(data = tmp, aes(k, wss)) +
      geom_line() +
      geom_segment(aes(x = k[1], y = wss[1],
                       xend = max(k), yend = wss[which.max(k)]),
                   linetype = "dashed") +
      geom_point(aes(size = (d == max(d)), color = (d == max(d))),
                 show.legend = FALSE) +
      scale_size_manual(values = c(2,5)) +
      scale_color_manual(values = c("black", "red")) +
      labs(x = "Number of clusters",
           y = "Total within-cluster sum of squares",
           title = "Elbow plot for the hierarchical method") +
      theme_bw()
  }
}

# Função vizualização dos grupos
# NÃO MUDAR!
cluster_viz <- function(data, clusters, 
                        axes = c(1, 2), geom = c("point", "text"), repel = TRUE, 
                        show.clust.cent = TRUE, ellipse = TRUE, ellipse.type = "convex", 
                        ellipse.level = 0.95, ellipse.alpha = 0.2, shape = NULL, 
                        pointsize = 1.5, labelsize = 12, main = "Cluster plot",
                        ggtheme = theme_bw()) {
  require(factoextra)
  data <- scale(data)
  pca <- stats::prcomp(data, scale = FALSE, center = FALSE)
  ind <- facto_summarize(pca, element = "ind", result = "coord", axes = axes)
  eig <- get_eigenvalue(pca)[axes, 2]
  xlab <- paste0("Dim", axes[1], " (", round(eig[1], 1), "%)")
  ylab <- paste0("Dim", axes[2], " (", round(eig[2], 1), "%)")
  colnames(ind)[2:3] <- c("x", "y")
  label_coord <- ind
  lab <- NULL
  if ("text" %in% geom) 
    lab <- "name"
  if (is.null(shape)) 
    shape <- "cluster"
  plot.data <- cbind.data.frame(ind, cluster = clusters, stringsAsFactors = TRUE)
  label_coord <- cbind.data.frame(label_coord, cluster = clusters, stringsAsFactors = TRUE)
  p <- ggpubr::ggscatter(plot.data, "x", "y", color = "cluster", 
                         shape = shape, size = pointsize, point = "point" %in% geom,
                         label = lab, font.label = labelsize, repel = repel, 
                         mean.point = show.clust.cent, ellipse = ellipse, ellipse.type = ellipse.type, 
                         ellipse.alpha = ellipse.alpha, ellipse.level = ellipse.level, 
                         main = main, xlab = xlab, ylab = ylab, ggtheme = ggtheme)
  p
}

# Encontrar número ótimo de grupos para o método hierárquico
set.seed(123456789)
p3 <- elbow.plot(dados.pad, alg = "hclust")
p3

# K "ótimo" é igual a 4. Mudar de acordo com o seu resultado

d <-dist(dados.pad, method = "euclidean")
h.fit <-hclust(d, method = "ward.D")
plot(h.fit, main='Dendograma - Método Hierárquico', xlab='Cluster das Observações - Distância Euclidiana e Método de Ward', ylab='Altura')

groups_a <- cutree(h.fit, k=3)
groups_a 

groups_b <- cutree(h.fit, k=4)
groups_b 

groups_c <- cutree(h.fit, k=5)
dendogram3 <- rect.hclust(h.fit, k=5, border="red")
dendogram3

summary(fatorial2$scores)
attach(dados)
by(dados, groups_c, summary)

# Agrupamento usando o método hierárquico - 2 forma
nclust = 5 # Número de grupos de acordo com os gráficos
fit.hclust <- hcut(dados.pad, 5) # Ajustar K de acordo com o gráfico
fit.hclust

g_cluster <- cluster_viz(dados.pad, as.factor(fit.hclust$cluster), geom = "point", main = "Gráfico Cluster para o Método Hierárquico")
plot(g_cluster)

```

## Métodos Não-Hierárquicos

São métodos de partição de "n" objetos em "k" grupos. Os "k" grupos são pré-determinados. A cada passo verifica-se se os objetos estão alocados da "melhor" maneira. Função objetivo: procura minimizar a variância dentro do grupo e maximizar a variância entre os grupos. 

Não fornece dendograma. Isto porque se em algum passo do algoritmo dois elementos tiverem sido colocados num mesmo cluster, não necessariamente "estarão juntos" na partição final. Bom quando se tem um grande número de observações. O problema principal aqui não é o número de grupos e sim a melhor forma de alocar os elementos em k grupos. O método mais usado é o das k-médias (k-means).

Resumidamente, o método das k-means é composto de quatro passos. No primeiro são selecionados k centróides para inicializar o processo de partição. Cada elemento do conjunto de dados é, então, comparado com cada centróide inicial, através de uma medida de distância que, em geral, é a Euclidiana. O elemento é alocado ao grupo cuja distância é a menor. No terceiro passo, recalcula-se os valores dos centróides para cada novo grupo formado. Com estes novos centróides, repete-se o passo 2. Isto deve ser repetido até que nenhuma realocação de elementos seja necessária. 


```  {r estat9, warning=FALSE, message=FALSE}
cluster_kmeans <- elbow.plot(dados.pad)
cluster_kmeans

kmeans1 <- kmeans(dados.pad, centers=5)
#Centróides
kmeans1$centers

#Plot dos Clusters
clusplot(dados.pad, kmeans1$cluster, main="2D Representação da solução dos Clusters",
         color=TRUE, shade=TRUE, labels=2, lines=0)

# Agrupamento usando o K-means - 2 forma
fit.kmeans <- kmeans(dados.pad, 5) # Ajustar K de acordo com o gráfico
fit.kmeans
g_kmeans <- cluster_viz(dados.pad, as.factor(fit.kmeans$cluster), geom = "point",
                  main = "Gráfico Cluster para o Método K-médias")
plot(g_kmeans)
```
